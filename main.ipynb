{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set all paths and parameters in this cell."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T15:53:05.929810Z",
     "start_time": "2025-11-21T15:53:05.922532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ======================================================================================\n",
    "# CONFIGURATION - Adjust these paths and parameters\n",
    "# ======================================================================================\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = \"/media/thanakornbuath/data SSD/her2-attention-classifier/data\"\n",
    "OUTPUTS_ROOT = \"/media/thanakornbuath/data SSD/her2-attention-classifier/outputs\"\n",
    "ZARR_OUTPUT_DIR = \"/media/thanakornbuath/patch/zarr_norm\"  # CHANGE THIS\n",
    "PATCHES_ROOT = f\"{OUTPUTS_ROOT}/patches\"  # For reference sampling\n",
    "\n",
    "# Reference stain normalization\n",
    "REF_STAIN_STATS_PATH = f\"{OUTPUTS_ROOT}/ref_stain_stats.npz\"\n",
    "NUM_REF_SUBFOLDERS = 100   # Number of random subfolders to sample\n",
    "IMAGES_PER_FOLDER = 200  # Max images per sampled subfolder (reduced to prevent OOM)\n",
    "\n",
    "# Patch extraction parameters\n",
    "PATCH_SIZE = 512\n",
    "STRIDE = 512  # No overlap\n",
    "LEVEL = 0  # Highest resolution\n",
    "TISSUE_THRESHOLD = 0.2  # Minimum 20% tissue in patch (via quick HSV-based check)\n",
    "DOWNSAMPLE_MASK = 1  # Downsample mask for memory efficiency\n",
    "\n",
    "# Performance parameters\n",
    "NUM_WORKERS = 8  # Parallel patch extraction workers (threads)\n",
    "BATCH_SIZE = 128  # Patches per batch write to Zarr\n",
    "USE_GPU = True  # GPU acceleration for Macenko normalization (CuPy)\n",
    "SKIP_EXISTING = True  # Skip existing .zarr files\n",
    "\n",
    "# Dataset selection\n",
    "COHORTS = [\n",
    "    \"TCGA_BRCA_Filtered\",\n",
    "    \"Yale_HER2_cohort\",\n",
    "    \"Yale_trastuzumab_response_cohort\"\n",
    "]\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Data root: {DATA_ROOT}\")\n",
    "print(f\"  Zarr output: {ZARR_OUTPUT_DIR}\")\n",
    "print(f\"  Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\n",
    "print(f\"  Workers: {NUM_WORKERS}, GPU: {USE_GPU}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Data root: /media/thanakornbuath/data SSD/her2-attention-classifier/data\n",
      "  Zarr output: /media/thanakornbuath/patch/zarr_norm\n",
      "  Patch size: 512x512\n",
      "  Workers: 8, GPU: True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Environment"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T15:53:06.777464Z",
     "start_time": "2025-11-21T15:53:05.946185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import zarr\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(\"/media/thanakornbuath/data SSD/her2-attention-classifier\")\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# External libs used during execution\n",
    "try:\n",
    "    import openslide\n",
    "except Exception as e:\n",
    "    print(\"⚠ openslide not available. Please install openslide-python and libopenslide.\")\n",
    "    raise\n",
    "\n",
    "# Setup logging\n",
    "os.makedirs(Path(OUTPUTS_ROOT) / \"logs\", exist_ok=True)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(f\"{OUTPUTS_ROOT}/logs/main_preprocessing.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(ZARR_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"✓ Environment setup complete\")\n",
    "print(f\"  Python version: {sys.version.split()[0]}\")\n",
    "\n",
    "# Check GPU\n",
    "if USE_GPU:\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        _gpu_count = cp.cuda.runtime.getDeviceCount()\n",
    "        print(f\"  ✓ CuPy available: {_gpu_count} GPU(s)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ CuPy not available ({e}), using CPU\")\n",
    "        USE_GPU = False\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment setup complete\n",
      "  Python version: 3.12.0\n",
      "  ✓ CuPy available: 1 GPU(s)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Stain Normalization (Macenko) - GPU optional"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T15:53:07.561402Z",
     "start_time": "2025-11-21T15:53:07.552108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "\n",
    "class MacenkoNormalizer:\n",
    "    \"\"\"Macenko stain normalization with optional CuPy acceleration for linear algebra ops.\n",
    "    Only matrix ops are on GPU; I/O and conversions stay on CPU.\n",
    "    \"\"\"\n",
    "    def __init__(self, percentiles: Tuple[float, float]=(1, 99), use_gpu: bool=False):\n",
    "        self.percentiles = percentiles\n",
    "        self.use_gpu = use_gpu\n",
    "        self.xp = cp if (use_gpu and 'cp' in globals()) else np\n",
    "\n",
    "    @staticmethod\n",
    "    def _rgb_to_od(image_rgb: np.ndarray) -> np.ndarray:\n",
    "        img = image_rgb.astype(np.float32) + 1.0  # avoid log(0), 1/255 is negligible\n",
    "        od = -np.log(img / 255.0)\n",
    "        return od\n",
    "\n",
    "    @staticmethod\n",
    "    def _od_to_rgb(image_od: np.ndarray) -> np.ndarray:\n",
    "        rgb = (255.0 * np.exp(-image_od)).clip(0, 255).astype(np.uint8)\n",
    "        return rgb\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_cpu(arr):\n",
    "        try:\n",
    "            import cupy as cp\n",
    "            if isinstance(arr, cp.ndarray):\n",
    "                return cp.asnumpy(arr)\n",
    "        except Exception:\n",
    "            pass\n",
    "        return np.asarray(arr)\n",
    "\n",
    "    def _get_stain_vectors_and_concentrations(self, image_rgb: np.ndarray) -> Tuple[np.ndarray, np.ndarray, Tuple[int,int,int]]:\n",
    "        # Convert to OD\n",
    "        od = self._rgb_to_od(image_rgb)\n",
    "        H, W, CH = od.shape\n",
    "        od_reshaped = od.reshape(-1, 3)\n",
    "\n",
    "        # Filter near-white background\n",
    "        mask = np.sum(od_reshaped, axis=1) > 0.2\n",
    "        od_filtered = od_reshaped[mask]\n",
    "        if od_filtered.shape[0] < 100:\n",
    "            # too few pixels; fallback to using all\n",
    "            od_filtered = od_reshaped\n",
    "\n",
    "        # Move to xp for PCA\n",
    "        xp = self.xp\n",
    "        odf = xp.asarray(od_filtered, dtype=xp.float32)\n",
    "        # Center\n",
    "        odf = odf - odf.mean(axis=0, keepdims=True)\n",
    "        # SVD to get top 2 PCs\n",
    "        try:\n",
    "            U, S, VT = xp.linalg.svd(odf, full_matrices=False)\n",
    "            v = VT.T[:, :2]  # (3,2)\n",
    "            # Explicitly delete large SVD matrices\n",
    "            del U, S, VT\n",
    "        except Exception:\n",
    "            # CPU fallback if GPU fails\n",
    "            U, S, VT = np.linalg.svd(od_filtered - od_filtered.mean(axis=0, keepdims=True), full_matrices=False)\n",
    "            v = VT.T[:, :2]\n",
    "            del U, S, VT\n",
    "            xp = np\n",
    "\n",
    "        # Normalize columns\n",
    "        v = v / xp.linalg.norm(v, axis=0, keepdims=True)\n",
    "\n",
    "        # Ensure consistent direction (positive sum)\n",
    "        for i in range(2):\n",
    "            if float(v[:, i].sum()) < 0:\n",
    "                v[:, i] = -v[:, i]\n",
    "\n",
    "        # CRITICAL: Macenko angle-based stain separation\n",
    "        # Project filtered OD onto the 2D plane spanned by top 2 PCs\n",
    "        odf_2d = xp.asarray(od_filtered, dtype=xp.float32) @ v  # (N_filtered, 2)\n",
    "\n",
    "        # Compute angles in 2D plane\n",
    "        angles = xp.arctan2(odf_2d[:, 1], odf_2d[:, 0])\n",
    "\n",
    "        # Find angle percentiles to define stain extremes (THIS WAS MISSING!)\n",
    "        angles_cpu = self._to_cpu(angles)\n",
    "        min_angle = float(np.percentile(angles_cpu, self.percentiles[0]))\n",
    "        max_angle = float(np.percentile(angles_cpu, self.percentiles[1]))\n",
    "\n",
    "        # Construct stain vectors at these extreme angles\n",
    "        # min_angle → Hematoxylin (typically more bluish, first component dominant)\n",
    "        # max_angle → Eosin (typically more pinkish, second component dominant)\n",
    "        stain_h = xp.cos(min_angle) * v[:, 0] + xp.sin(min_angle) * v[:, 1]\n",
    "        stain_e = xp.cos(max_angle) * v[:, 0] + xp.sin(max_angle) * v[:, 1]\n",
    "\n",
    "        # Normalize stain vectors\n",
    "        stain_h = stain_h / xp.linalg.norm(stain_h)\n",
    "        stain_e = stain_e / xp.linalg.norm(stain_e)\n",
    "\n",
    "        # Stack into matrix [H, E] as columns\n",
    "        stain_matrix = xp.column_stack([stain_h, stain_e])\n",
    "\n",
    "        # Project ALL pixels (od_reshaped) onto these true stain vectors\n",
    "        od_all = xp.asarray(od_reshaped.astype(np.float32))\n",
    "        C = od_all @ stain_matrix  # (N,2) - now correctly separated H&E concentrations\n",
    "        C = xp.maximum(C, 0)  # non-negative\n",
    "\n",
    "        # Back to CPU explicitly (avoid implicit CuPy->NumPy conversion)\n",
    "        stain_vectors = self._to_cpu(stain_matrix)\n",
    "        concentrations = self._to_cpu(C)\n",
    "\n",
    "        # Cleanup intermediate arrays\n",
    "        del odf_2d, angles, stain_h, stain_e, stain_matrix\n",
    "\n",
    "        # Explicitly delete GPU arrays to free memory immediately\n",
    "        del odf, od_all, v\n",
    "        if self.use_gpu:\n",
    "            try:\n",
    "                del C  # Delete GPU version\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return stain_vectors, concentrations, (H, W, CH)\n",
    "\n",
    "    def get_mean_reference_stain_characteristics(self, list_of_reference_images_rgb: List[np.ndarray]):\n",
    "        if not list_of_reference_images_rgb:\n",
    "            raise ValueError(\"list_of_reference_images_rgb cannot be empty.\")\n",
    "        all_V = []\n",
    "        max_h = []\n",
    "        max_e = []\n",
    "        for i, img in enumerate(list_of_reference_images_rgb):\n",
    "            V, C, _ = self._get_stain_vectors_and_concentrations(img)\n",
    "            all_V.append(V)\n",
    "            # Extract percentiles and immediately delete huge C array\n",
    "            h_val = float(np.percentile(C[:, 0], self.percentiles[1]))\n",
    "            e_val = float(np.percentile(C[:, 1], self.percentiles[1]))\n",
    "            max_h.append(h_val)\n",
    "            max_e.append(e_val)\n",
    "            del C, V  # Free large arrays immediately\n",
    "\n",
    "            # Free GPU memory every 10 images to prevent pool growth\n",
    "            if self.use_gpu and (i + 1) % 10 == 0:\n",
    "                try:\n",
    "                    import cupy as cp\n",
    "                    cp.get_default_memory_pool().free_all_blocks()\n",
    "                    cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            # Force garbage collection every 50 images\n",
    "            if (i + 1) % 50 == 0:\n",
    "                gc.collect()\n",
    "\n",
    "        # Compute final statistics\n",
    "        mean_V = np.mean(np.stack(all_V, axis=0), axis=0)\n",
    "        mean_V = mean_V / np.linalg.norm(mean_V, axis=0, keepdims=True)\n",
    "        mean_max_h = float(np.mean(max_h))\n",
    "        mean_max_e = float(np.mean(max_e))\n",
    "\n",
    "        # Clean up intermediate lists\n",
    "        del all_V, max_h, max_e\n",
    "        gc.collect()\n",
    "\n",
    "        # Final GPU cleanup\n",
    "        if self.use_gpu:\n",
    "            try:\n",
    "                import cupy as cp\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "                cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return mean_V, (mean_max_h, mean_max_e)\n",
    "\n",
    "    def normalize(self, target_image_rgb: np.ndarray,\n",
    "                  mean_ref_stain_vectors: np.ndarray,\n",
    "                  mean_ref_max_concentrations_tuple: Tuple[float, float]) -> np.ndarray:\n",
    "        # Target characteristics\n",
    "        V_t, C_t, shape = self._get_stain_vectors_and_concentrations(target_image_rgb)\n",
    "        max_t_h = np.percentile(C_t[:, 0], self.percentiles[1])\n",
    "        max_t_e = np.percentile(C_t[:, 1], self.percentiles[1])\n",
    "\n",
    "        # Scale concentrations to reference\n",
    "        ref_max_h, ref_max_e = mean_ref_max_concentrations_tuple\n",
    "        scale_h = ref_max_h / (max_t_h + 1e-6)\n",
    "        scale_e = ref_max_e / (max_t_e + 1e-6)\n",
    "        Cn = C_t.copy()\n",
    "        Cn[:, 0] *= scale_h\n",
    "        Cn[:, 1] *= scale_e\n",
    "        Cn = np.maximum(Cn, 0)\n",
    "\n",
    "        # Reconstruct OD using reference stain vectors\n",
    "        V_ref = mean_ref_stain_vectors.astype(np.float32)\n",
    "        od_norm = (Cn @ V_ref.T).reshape(shape)\n",
    "        rgb_norm = self._od_to_rgb(od_norm)\n",
    "        return rgb_norm\n",
    "\n",
    "# Utility to load reference params from npz\n",
    "def load_reference_stain_params(npz_path: Path, use_gpu: bool=False) -> Optional[Dict]:\n",
    "    try:\n",
    "        if not npz_path.exists():\n",
    "            return None\n",
    "        data = np.load(str(npz_path))\n",
    "        if 'stain_vectors' in data and ('max_h' in data or 'mean_max_h' in data):\n",
    "            V = data['stain_vectors']\n",
    "            max_h = float(data.get('max_h', data.get('mean_max_h')))\n",
    "            max_e = float(data.get('max_e', data.get('mean_max_e')))\n",
    "            return {\n",
    "                'stain_vectors': V,\n",
    "                'max_concentrations': (max_h, max_e),\n",
    "                'use_gpu': use_gpu,\n",
    "                'percentiles': (1, 99)\n",
    "            }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load reference stain parameters: {e}\")\n",
    "    return None\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Compute Reference Stain Statistics\n",
    "\n",
    "Sample images from existing patches and compute mean Macenko stain characteristics."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T16:00:47.123114Z",
     "start_time": "2025-11-21T15:53:07.662554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def load_images_from_folder(folder_path: Path, max_images: int = 10) -> List[np.ndarray]:\n",
    "    images: List[np.ndarray] = []\n",
    "    supported_ext = ('.png', '.jpg', '.jpeg', '.tif', '.tiff')\n",
    "    if not folder_path.exists():\n",
    "        return images\n",
    "    # gather files (non-recursive, matches original request: per subfolder)\n",
    "    image_files: List[Path] = []\n",
    "    for ext in supported_ext:\n",
    "        image_files.extend(list(folder_path.glob(f\"*{ext}\")))\n",
    "        image_files.extend(list(folder_path.glob(f\"*{ext.upper()}\")))\n",
    "    if len(image_files) == 0:\n",
    "        return images\n",
    "    if len(image_files) > max_images:\n",
    "        image_files = random.sample(image_files, max_images)\n",
    "    for p in image_files:\n",
    "        try:\n",
    "            with Image.open(p) as img:\n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "                images.append(np.array(img))\n",
    "        except Exception as e:\n",
    "            logging.debug(f\"Skip {p}: {e}\")\n",
    "    return images\n",
    "\n",
    "# Check if reference stats already exist\n",
    "ref_stats_path = Path(REF_STAIN_STATS_PATH)\n",
    "if ref_stats_path.exists():\n",
    "    print(f\"✓ Reference stats already exist at {REF_STAIN_STATS_PATH}\")\n",
    "    try:\n",
    "        data = np.load(ref_stats_path)\n",
    "        keys = list(data.keys())\n",
    "        print(f\"  Keys: {keys}\")\n",
    "        if 'max_h' in keys:\n",
    "            print(f\"  Max H: {float(data['max_h']):.4f}, Max E: {float(data['max_e']):.4f}\")\n",
    "        elif 'mean_max_h' in keys:\n",
    "            print(f\"  Max H: {float(data['mean_max_h']):.4f}, Max E: {float(data['mean_max_e']):.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠ Could not load stats: {e}. Will regenerate...\")\n",
    "        ref_stats_path = None\n",
    "else:\n",
    "    print(\"Reference stats not found. Computing from patches...\")\n",
    "    patches_root = Path(PATCHES_ROOT)\n",
    "    if not patches_root.exists():\n",
    "        print(f\"⚠ Patches root not found: {PATCHES_ROOT}\\n  Skipping reference computation.\")\n",
    "    else:\n",
    "        # Sample reference images\n",
    "        print(f\"Sampling from up to {NUM_REF_SUBFOLDERS} random folders...\")\n",
    "        subfolders = [d for d in patches_root.iterdir() if d.is_dir()]\n",
    "        num_to_sample = min(NUM_REF_SUBFOLDERS, len(subfolders))\n",
    "        random.seed(42)\n",
    "        sampled_folders = random.sample(subfolders, num_to_sample) if num_to_sample > 0 else []\n",
    "\n",
    "        all_reference_images: List[np.ndarray] = []\n",
    "        print(f\"Loading reference images from {len(sampled_folders)} folders (max {IMAGES_PER_FOLDER} per folder)...\")\n",
    "        for folder_idx, folder in enumerate(tqdm(sampled_folders, desc=\"Loading reference images\")):\n",
    "            imgs = load_images_from_folder(folder, max_images=IMAGES_PER_FOLDER)\n",
    "            all_reference_images.extend(imgs)\n",
    "            del imgs  # Free folder batch immediately\n",
    "\n",
    "            # Force garbage collection every 5 folders to prevent accumulation\n",
    "            if (folder_idx + 1) % 5 == 0:\n",
    "                gc.collect()\n",
    "\n",
    "\n",
    "        print(f\"✓ Loaded {len(all_reference_images)} reference images\")\n",
    "        if len(all_reference_images) > 0:\n",
    "            print(\"Computing Macenko reference statistics (with aggressive memory management)...\")\n",
    "            normalizer = MacenkoNormalizer(use_gpu=USE_GPU, percentiles=(1, 99))\n",
    "\n",
    "            # Compute with progress tracking\n",
    "            print(f\"  Processing {len(all_reference_images)} images for stain characteristics...\")\n",
    "            mean_V, (mean_max_h, mean_max_e) = normalizer.get_mean_reference_stain_characteristics(all_reference_images)\n",
    "\n",
    "            # Save\n",
    "            ref_stats_path = Path(REF_STAIN_STATS_PATH)\n",
    "            ref_stats_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            np.savez(ref_stats_path, stain_vectors=mean_V, max_h=mean_max_h, max_e=mean_max_e)\n",
    "\n",
    "            print(f\"✓ Reference statistics saved to {REF_STAIN_STATS_PATH}\")\n",
    "            print(f\"  Max H: {mean_max_h:.4f}, Max E: {mean_max_e:.4f}\")\n",
    "\n",
    "            # Explicitly free large reference image list to avoid memory growth in notebook sessions\n",
    "            del all_reference_images\n",
    "            del mean_V\n",
    "            del normalizer\n",
    "            gc.collect()\n",
    "\n",
    "            # Final GPU cleanup\n",
    "            if USE_GPU:\n",
    "                try:\n",
    "                    import cupy as cp\n",
    "                    cp.get_default_memory_pool().free_all_blocks()\n",
    "                    cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "                    print(\"  ✓ GPU memory pools freed\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ⚠ GPU cleanup warning: {e}\")\n",
    "        else:\n",
    "            print(\"⚠ No reference images found; proceeding without normalization\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference stats not found. Computing from patches...\n",
      "Sampling from up to 100 random folders...\n",
      "Loading reference images from 100 folders (max 200 per folder)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading reference images:   0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c901ba81b1654df88b0306a488a6ba14"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 15889 reference images\n",
      "Computing Macenko reference statistics (with aggressive memory management)...\n",
      "  Processing 15889 images for stain characteristics...\n",
      "✓ Reference statistics saved to /media/thanakornbuath/data SSD/her2-attention-classifier/outputs/ref_stain_stats.npz\n",
      "  Max H: 2.4669, Max E: 2.4352\n",
      "  ✓ GPU memory pools freed\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Load Reference Stain Parameters"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T16:00:47.228627Z",
     "start_time": "2025-11-21T16:00:47.224398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "normalizer_params = load_reference_stain_params(Path(REF_STAIN_STATS_PATH), use_gpu=USE_GPU)\n",
    "if normalizer_params is None:\n",
    "    print(\"⚠️  Warning: Reference stain parameters not loaded! Patches will NOT be stain normalized.\")\n",
    "else:\n",
    "    print(\"✓ Loaded reference stain parameters\")\n",
    "    print(f\"  Max H: {normalizer_params['max_concentrations'][0]:.4f}\")\n",
    "    print(f\"  Max E: {normalizer_params['max_concentrations'][1]:.4f}\")\n",
    "    print(f\"  GPU: {normalizer_params['use_gpu']}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded reference stain parameters\n",
      "  Max H: 2.4669\n",
      "  Max E: 2.4352\n",
      "  GPU: True\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Slide Discovery (SVS + XML)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T16:00:47.296387Z",
     "start_time": "2025-11-21T16:00:47.271893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def discover_slides(data_root: str, cohorts: list):\n",
    "    slides = []\n",
    "    for cohort in cohorts:\n",
    "        cohort_dir = Path(data_root) / cohort\n",
    "        svs_dir = cohort_dir / \"SVS\"\n",
    "        xml_dir = cohort_dir / \"Annotations\"\n",
    "        if not svs_dir.exists():\n",
    "            print(f\"⚠️  SVS directory not found: {svs_dir}\")\n",
    "            continue\n",
    "        if not xml_dir.exists():\n",
    "            print(f\"⚠️  Annotations directory not found: {xml_dir}\")\n",
    "            continue\n",
    "        # Optional labels\n",
    "        labels_dict = {}\n",
    "        for label_file_name in ['labels.csv', 'HER2_TCGA_clean.csv']:\n",
    "            label_file = cohort_dir / label_file_name\n",
    "            if label_file.exists():\n",
    "                try:\n",
    "                    df_labels = pd.read_csv(label_file)\n",
    "                    if 'slide_id' in df_labels.columns and 'label' in df_labels.columns:\n",
    "                        labels_dict = dict(zip(df_labels['slide_id'], df_labels['label']))\n",
    "                    elif 'case_id' in df_labels.columns and 'HER2_IHC_Status' in df_labels.columns:\n",
    "                        def map_her2_status(status):\n",
    "                            if isinstance(status, str) and (('Positive' in status) or ('3+' in status) or ('2+' in status)):\n",
    "                                return 1\n",
    "                            return 0\n",
    "                        labels_dict = {row['case_id']: map_her2_status(row['HER2_IHC_Status']) for _, row in df_labels.iterrows()}\n",
    "                    print(f\"✓ Loaded labels for {len(labels_dict)} slides from {cohort}/{label_file_name}\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️  Failed to load labels from {label_file}: {e}\")\n",
    "        # SVS files\n",
    "        svs_files = list(svs_dir.glob(\"*.svs\")) + list(svs_dir.glob(\"*.SVS\"))\n",
    "        for svs_path in svs_files:\n",
    "            slide_name = svs_path.stem\n",
    "\n",
    "            # For TCGA slides (e.g., TCGA-XX-YYYY.01234.svs), use only part before first dot\n",
    "            # to match XML (e.g., TCGA-XX-YYYY.xml)\n",
    "            xml_base_name = slide_name\n",
    "            if slide_name.startswith(\"TCGA-\") and \".\" in slide_name:\n",
    "                xml_base_name = slide_name.split(\".\")[0]\n",
    "\n",
    "            xml_candidates = [\n",
    "                xml_dir / f\"{xml_base_name}.xml\",\n",
    "                xml_dir / f\"{xml_base_name}.XML\",\n",
    "                xml_dir / f\"{slide_name}.xml\",  # Also try full name as fallback\n",
    "                xml_dir / f\"{slide_name}.XML\"\n",
    "            ]\n",
    "            xml_path = None\n",
    "            for cand in xml_candidates:\n",
    "                if cand.exists():\n",
    "                    xml_path = cand\n",
    "                    break\n",
    "            if xml_path is None:\n",
    "                print(f\"⚠️  No XML found for {slide_name} (tried {xml_base_name}), skipping\")\n",
    "                continue\n",
    "            label = int(labels_dict.get(slide_name, 0))\n",
    "            slides.append({\n",
    "                'slide_id': slide_name,\n",
    "                'svs_path': str(svs_path),\n",
    "                'xml_path': str(xml_path),\n",
    "                'cohort': cohort,\n",
    "                'label': label\n",
    "            })\n",
    "    return slides\n",
    "\n",
    "slides = discover_slides(DATA_ROOT, COHORTS)\n",
    "print(f\"\\n✓ Discovered {len(slides)} slides across {len(COHORTS)} cohorts\")\n",
    "print(f\"\\nCohort breakdown:\")\n",
    "for cohort in COHORTS:\n",
    "    count = sum(1 for s in slides if s['cohort'] == cohort)\n",
    "    print(f\"  {cohort}: {count} slides\")\n",
    "if slides:\n",
    "    labels = [s['label'] for s in slides]\n",
    "    print(f\"\\nLabel distribution: HER2- (0): {sum(1 for l in labels if l == 0)}, HER2+ (1): {sum(1 for l in labels if l == 1)}\")\n",
    "else:\n",
    "    print(\"\\n⚠️  No slides discovered! Check data directory structure.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded labels for 0 slides from TCGA_BRCA_Filtered/HER2_TCGA_clean.csv\n",
      "⚠️  No XML found for Her2Neg_Case_34 (tried Her2Neg_Case_34), skipping\n",
      "⚠️  No XML found for Her2Pos_Case_27 (tried Her2Pos_Case_27), skipping\n",
      "⚠️  No XML found for Her2Pos_Case_52 (tried Her2Pos_Case_52), skipping\n",
      "⚠️  No XML found for Her2Pos_Case_45 (tried Her2Pos_Case_45), skipping\n",
      "⚠️  No XML found for Her2Neg_Case_31 (tried Her2Neg_Case_31), skipping\n",
      "\n",
      "✓ Discovered 454 slides across 3 cohorts\n",
      "\n",
      "Cohort breakdown:\n",
      "  TCGA_BRCA_Filtered: 182 slides\n",
      "  Yale_HER2_cohort: 187 slides\n",
      "  Yale_trastuzumab_response_cohort: 85 slides\n",
      "\n",
      "Label distribution: HER2- (0): 454, HER2+ (1): 0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. XML → Mask, Grid Sampling, and Zarr Writer"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T16:00:47.334798Z",
     "start_time": "2025-11-21T16:00:47.320417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Simple HSV tissue filter (fast) to exclude whitespace patches\n",
    "def tissue_fraction_rgb(patch_rgb: np.ndarray) -> float:\n",
    "    # Convert to HSV via matplotlib (fast enough)\n",
    "    import matplotlib.colors as mcolors\n",
    "    hsv = mcolors.rgb_to_hsv(patch_rgb.astype(np.float32) / 255.0)\n",
    "    s = hsv[..., 1]\n",
    "    v = hsv[..., 2]\n",
    "    tissue = (s > 0.1) & (v < 0.95)\n",
    "    return float(tissue.mean())\n",
    "\n",
    "# Parse Aperio-like XML polygons\n",
    "def parse_xml_polygons(xml_path: str) -> List[np.ndarray]:\n",
    "    polys: List[np.ndarray] = []\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        # Look for Regions/Vertices/Vertex\n",
    "        for region in root.iter(\"Region\"):\n",
    "            pts = []\n",
    "            vertices = list(region.iter(\"Vertex\"))\n",
    "            if not vertices:\n",
    "                # Some schemas use 'Coordinate'\n",
    "                vertices = list(region.iter(\"Coordinate\"))\n",
    "            for v in vertices:\n",
    "                x = v.get('X') or v.get('x') or v.get('XCoord')\n",
    "                y = v.get('Y') or v.get('y') or v.get('YCoord')\n",
    "                if x is None or y is None:\n",
    "                    continue\n",
    "                pts.append((float(x), float(y)))\n",
    "            if len(pts) >= 3:\n",
    "                polys.append(np.array(pts, dtype=np.float32))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to parse XML {xml_path}: {e}\")\n",
    "    return polys\n",
    "\n",
    "# Rasterize polygons into a downsampled mask\n",
    "def polygons_to_mask(polygons: List[np.ndarray], level0_size: Tuple[int,int], downsample: int=16) -> np.ndarray:\n",
    "    W, H = level0_size  # OpenSlide gives (width,height)\n",
    "    w_small = max(1, W // downsample)\n",
    "    h_small = max(1, H // downsample)\n",
    "    mask_img = Image.new('L', (w_small, h_small), 0)\n",
    "    draw = ImageDraw.Draw(mask_img)\n",
    "    for poly in polygons:\n",
    "        scaled = [(p[0]/downsample, p[1]/downsample) for p in poly]\n",
    "        try:\n",
    "            draw.polygon(scaled, outline=1, fill=1)\n",
    "        except Exception:\n",
    "            # If polygon invalid, skip\n",
    "            continue\n",
    "    return (np.array(mask_img) > 0)\n",
    "\n",
    "# Generate grid centers at level 0\n",
    "def generate_grid_centers(level0_size: Tuple[int,int], patch: int, stride: int) -> List[Tuple[int,int]]:\n",
    "    W, H = level0_size\n",
    "    xs = list(range(patch//2, W - patch//2 + 1, stride))\n",
    "    ys = list(range(patch//2, H - patch//2 + 1, stride))\n",
    "    centers = [(x, y) for y in ys for x in xs]\n",
    "    return centers\n",
    "\n",
    "# Create Zarr group and arrays\n",
    "def create_zarr_group(zarr_path: Path, num_patches: int, patch: int) -> zarr.hierarchy.Group:\n",
    "    store = zarr.DirectoryStore(str(zarr_path))\n",
    "    root = zarr.group(store=store, overwrite=True)\n",
    "    compressor = zarr.Blosc(cname='zstd', clevel=5, shuffle=2)\n",
    "    # allow resize after filtering by setting maxshape None on first dimension\n",
    "    root.create_dataset('patches', shape=(num_patches, patch, patch, 3), maxshape=(None, patch, patch, 3), chunks=(min(256, num_patches), patch, patch, 3), dtype='u1', compressor=compressor, overwrite=True)\n",
    "    root.create_dataset('coords', shape=(num_patches, 2), maxshape=(None, 2), chunks=(min(4096, num_patches), 2), dtype='i4', compressor=compressor, overwrite=True)\n",
    "    root.create_dataset('labels', shape=(num_patches,), maxshape=(None,), chunks=(min(4096, num_patches),), dtype='i1', compressor=compressor, overwrite=True)\n",
    "    return root\n",
    "\n",
    "# Process a single slide → Zarr\n",
    "def process_slide(slide_info: dict,\n",
    "                  patch_size: int,\n",
    "                  stride: int,\n",
    "                  level: int,\n",
    "                  tissue_threshold: float,\n",
    "                  downsample_mask: int,\n",
    "                  normalizer_params: Optional[Dict],\n",
    "                  out_dir: Path,\n",
    "                  num_workers: int=4,\n",
    "                  batch_size: int=128) -> bool:\n",
    "    slide_id = slide_info['slide_id']\n",
    "    svs_path = slide_info['svs_path']\n",
    "    xml_path = slide_info['xml_path']\n",
    "    label = int(slide_info['label'])\n",
    "\n",
    "    zarr_path = out_dir / f\"{slide_id}.zarr\"\n",
    "    if zarr_path.exists() and SKIP_EXISTING:\n",
    "        # ensure it's complete by checking meta.json\n",
    "        meta_ok = (zarr_path / 'meta.json').exists()\n",
    "        if meta_ok:\n",
    "            logging.info(f\"Skip existing: {slide_id}\")\n",
    "            return True\n",
    "        else:\n",
    "            logging.info(f\"Existing zarr missing meta; rewriting: {slide_id}\")\n",
    "\n",
    "    # Open slide\n",
    "    slide = None\n",
    "    try:\n",
    "        slide = openslide.OpenSlide(svs_path)\n",
    "        W, H = slide.level_dimensions[0]\n",
    "        # XML polygons → mask\n",
    "        polygons = parse_xml_polygons(xml_path)\n",
    "        if len(polygons) == 0:\n",
    "            logging.warning(f\"No polygons in XML for {slide_id}; skipping\")\n",
    "            slide.close()\n",
    "            return False\n",
    "        mask = polygons_to_mask(polygons, (W, H), downsample=downsample_mask)\n",
    "        # Grid centers\n",
    "        centers = generate_grid_centers((W, H), patch_size, stride)\n",
    "        # Keep centers inside mask\n",
    "        ds = downsample_mask\n",
    "        valid_centers = [(x, y) for (x, y) in centers if mask[min(H//ds-1, y//ds), min(W//ds-1, x//ds)]]\n",
    "        if len(valid_centers) == 0:\n",
    "            logging.warning(f\"No valid centers after masking for {slide_id}; skipping\")\n",
    "            slide.close()\n",
    "            return False\n",
    "\n",
    "        # Create normalizer if params available\n",
    "        normalizer = None\n",
    "        if normalizer_params is not None:\n",
    "            normalizer = MacenkoNormalizer(percentiles=normalizer_params.get('percentiles', (1,99)),\n",
    "                                           use_gpu=normalizer_params.get('use_gpu', False))\n",
    "            V_ref = normalizer_params['stain_vectors']\n",
    "            ref_max = normalizer_params['max_concentrations']\n",
    "        else:\n",
    "            V_ref = None\n",
    "            ref_max = None\n",
    "\n",
    "        # Pre-create zarr arrays\n",
    "        z = create_zarr_group(zarr_path, len(valid_centers), patch_size)\n",
    "\n",
    "        # Metadata (MPP, magnification)\n",
    "        mpp_x = slide.properties.get(openslide.PROPERTY_NAME_MPP_X)\n",
    "        mpp_y = slide.properties.get(openslide.PROPERTY_NAME_MPP_Y)\n",
    "        try:\n",
    "            magnification = float(slide.properties.get('aperio.AppMag') or slide.properties.get('openslide.objective-power') or 0)\n",
    "        except Exception:\n",
    "            magnification = None\n",
    "\n",
    "        # Producer-consumer: thread pool per batch\n",
    "        def read_and_process(idx_center_pair):\n",
    "            idx, (cx, cy) = idx_center_pair\n",
    "            x0 = cx - patch_size//2\n",
    "            y0 = cy - patch_size//2\n",
    "            # Read region returns RGBA; convert to RGB\n",
    "            region = slide.read_region((x0, y0), level, (patch_size, patch_size))\n",
    "            try:\n",
    "                region = region.convert('RGB')\n",
    "                patch = np.array(region)\n",
    "            finally:\n",
    "                region.close()\n",
    "            # Quick tissue filter\n",
    "            if tissue_threshold > 0:\n",
    "                if tissue_fraction_rgb(patch) < tissue_threshold:\n",
    "                    return idx, None, (cx, cy)\n",
    "            # Normalize if available\n",
    "            if normalizer is not None and V_ref is not None and ref_max is not None:\n",
    "                try:\n",
    "                    patch = normalizer.normalize(patch, mean_ref_stain_vectors=V_ref, mean_ref_max_concentrations_tuple=ref_max)\n",
    "                except Exception as e:\n",
    "                    # If normalization fails, fall back to original patch\n",
    "                    logging.debug(f\"Norm fail at idx {idx}: {e}\")\n",
    "            return idx, patch, (cx, cy)\n",
    "\n",
    "        total = len(valid_centers)\n",
    "        written = 0\n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
    "            for start in tqdm(range(0, total, batch_size), total=math.ceil(total/batch_size), desc=f\"{slide_id}\"):\n",
    "                end = min(start + batch_size, total)\n",
    "                futures = [ex.submit(read_and_process, (i, valid_centers[i])) for i in range(start, end)]\n",
    "                for fut in as_completed(futures):\n",
    "                    idx, patch, coord = fut.result()\n",
    "                    if patch is None:\n",
    "                        continue\n",
    "                    # write sequentially to avoid holes and reduce disk\n",
    "                    z['patches'][written] = patch\n",
    "                    z['coords'][written] = coord\n",
    "                    z['labels'][written] = label\n",
    "                    written += 1\n",
    "                # Free GPU pools per batch to avoid leaks\n",
    "                if USE_GPU and 'cp' in globals():\n",
    "                    try:\n",
    "                        cp.get_default_memory_pool().free_all_blocks()\n",
    "                        cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                gc.collect()\n",
    "\n",
    "        # shrink datasets to actual written size to save disk/memory\n",
    "        try:\n",
    "            z['patches'].resize((written, patch_size, patch_size, 3))\n",
    "            z['coords'].resize((written, 2))\n",
    "            z['labels'].resize((written,))\n",
    "        except Exception as e:\n",
    "            logging.debug(f\"Resize failed for {slide_id}: {e}\")\n",
    "\n",
    "        # Write meta.json\n",
    "        meta = {\n",
    "            'slide_id': slide_id,\n",
    "            'label': label,\n",
    "            'num_patches': int(written),\n",
    "            'mpp_x': float(mpp_x) if mpp_x else None,\n",
    "            'mpp_y': float(mpp_y) if mpp_y else None,\n",
    "            'magnification': magnification,\n",
    "            'patch_size': patch_size,\n",
    "            'stride': stride,\n",
    "            'level': level\n",
    "        }\n",
    "        with open(zarr_path / 'meta.json', 'w') as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "\n",
    "        slide.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception processing slide {slide_id}: {e}\")\n",
    "        try:\n",
    "            if slide is not None:\n",
    "                slide.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "        return False\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Run SVS → Zarr Processing"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T16:01:33.035452Z",
     "start_time": "2025-11-21T16:00:47.387313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Starting preprocessing...\")\n",
    "print(f\"  Output directory: {ZARR_OUTPUT_DIR}\")\n",
    "print(f\"  Config: patch={PATCH_SIZE}, stride={STRIDE}, workers={NUM_WORKERS}, batch={BATCH_SIZE}, GPU={USE_GPU}, skip={SKIP_EXISTING}\")\n",
    "\n",
    "successful = 0\n",
    "failed = 0\n",
    "skipped = 0\n",
    "failed_slides = []\n",
    "\n",
    "for slide_info in tqdm(slides, desc=\"Processing slides\"):\n",
    "    zarr_path = Path(ZARR_OUTPUT_DIR) / f\"{slide_info['slide_id']}.zarr\"\n",
    "    if SKIP_EXISTING and zarr_path.exists() and (zarr_path / 'meta.json').exists():\n",
    "        skipped += 1\n",
    "        continue\n",
    "    ok = process_slide(\n",
    "        slide_info=slide_info,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        stride=STRIDE,\n",
    "        level=LEVEL,\n",
    "        tissue_threshold=TISSUE_THRESHOLD,\n",
    "        downsample_mask=DOWNSAMPLE_MASK,\n",
    "        normalizer_params=normalizer_params,\n",
    "        out_dir=Path(ZARR_OUTPUT_DIR),\n",
    "        num_workers=NUM_WORKERS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    if ok:\n",
    "        successful += 1\n",
    "    else:\n",
    "        failed += 1\n",
    "        failed_slides.append(slide_info['slide_id'])\n",
    "    if (successful + failed) % 3 == 0:\n",
    "        gc.collect()\n",
    "        if USE_GPU and 'cp' in globals():\n",
    "            try:\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "                cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Processing complete!\")\n",
    "print(f\"  Successful: {successful}\")\n",
    "print(f\"  Failed: {failed}\")\n",
    "print(f\"  Skipped (existing): {skipped}\")\n",
    "print(f\"  Total: {len(slides)}\")\n",
    "if failed_slides:\n",
    "    print(f\"Failed slides: {', '.join(failed_slides[:10])}{'...' if len(failed_slides) > 10 else ''}\")\n",
    "print(f\"{'='*60}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "  Output directory: /media/thanakornbuath/patch/zarr_norm\n",
      "  Config: patch=512, stride=512, workers=8, batch=128, GPU=True, skip=True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Processing slides:   0%|          | 0/454 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a8a9d8a8096461d93be9edd0c4443b6"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanakornbuath/anaconda3/envs/her2-class/lib/python3.12/site-packages/zarr/creation.py:190: UserWarning: ignoring keyword argument 'maxshape'\n",
      "  compressor, fill_value = _kwargs_compat(compressor, fill_value, kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TCGA-AN-A046-01Z-00-DX1.C529B94F-AFE3-4701-BC98-5D6EDF7B82C0:   0%|          | 0/14 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dac5dbf17ac945f9ac9b51d183b59f4b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 15\u001B[39m\n\u001B[32m     13\u001B[39m     skipped += \u001B[32m1\u001B[39m\n\u001B[32m     14\u001B[39m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m ok = \u001B[43mprocess_slide\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mslide_info\u001B[49m\u001B[43m=\u001B[49m\u001B[43mslide_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mPATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m=\u001B[49m\u001B[43mSTRIDE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mLEVEL\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtissue_threshold\u001B[49m\u001B[43m=\u001B[49m\u001B[43mTISSUE_THRESHOLD\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdownsample_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mDOWNSAMPLE_MASK\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnormalizer_params\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnormalizer_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m    \u001B[49m\u001B[43mout_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mZARR_OUTPUT_DIR\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_workers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mNUM_WORKERS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ok:\n\u001B[32m     28\u001B[39m     successful += \u001B[32m1\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 180\u001B[39m, in \u001B[36mprocess_slide\u001B[39m\u001B[34m(slide_info, patch_size, stride, level, tissue_threshold, downsample_mask, normalizer_params, out_dir, num_workers, batch_size)\u001B[39m\n\u001B[32m    178\u001B[39m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m    179\u001B[39m \u001B[38;5;66;03m# write sequentially to avoid holes and reduce disk\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m180\u001B[39m \u001B[43mz\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mpatches\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43mwritten\u001B[49m\u001B[43m]\u001B[49m = patch\n\u001B[32m    181\u001B[39m z[\u001B[33m'\u001B[39m\u001B[33mcoords\u001B[39m\u001B[33m'\u001B[39m][written] = coord\n\u001B[32m    182\u001B[39m z[\u001B[33m'\u001B[39m\u001B[33mlabels\u001B[39m\u001B[33m'\u001B[39m][written] = label\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/her2-class/lib/python3.12/site-packages/zarr/core.py:1451\u001B[39m, in \u001B[36mArray.__setitem__\u001B[39m\u001B[34m(self, selection, value)\u001B[39m\n\u001B[32m   1449\u001B[39m     \u001B[38;5;28mself\u001B[39m.set_orthogonal_selection(pure_selection, value, fields=fields)\n\u001B[32m   1450\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1451\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mset_basic_selection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpure_selection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfields\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfields\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/her2-class/lib/python3.12/site-packages/zarr/core.py:1547\u001B[39m, in \u001B[36mArray.set_basic_selection\u001B[39m\u001B[34m(self, selection, value, fields)\u001B[39m\n\u001B[32m   1545\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._set_basic_selection_zd(selection, value, fields=fields)\n\u001B[32m   1546\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1547\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_set_basic_selection_nd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mselection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfields\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfields\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/her2-class/lib/python3.12/site-packages/zarr/core.py:1937\u001B[39m, in \u001B[36mArray._set_basic_selection_nd\u001B[39m\u001B[34m(self, selection, value, fields)\u001B[39m\n\u001B[32m   1931\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_set_basic_selection_nd\u001B[39m(\u001B[38;5;28mself\u001B[39m, selection, value, fields=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m   1932\u001B[39m     \u001B[38;5;66;03m# implementation of __setitem__ for array with at least one dimension\u001B[39;00m\n\u001B[32m   1933\u001B[39m \n\u001B[32m   1934\u001B[39m     \u001B[38;5;66;03m# setup indexer\u001B[39;00m\n\u001B[32m   1935\u001B[39m     indexer = BasicIndexer(selection, \u001B[38;5;28mself\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1937\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_set_selection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfields\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfields\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/her2-class/lib/python3.12/site-packages/zarr/core.py:1990\u001B[39m, in \u001B[36mArray._set_selection\u001B[39m\u001B[34m(self, indexer, value, fields)\u001B[39m\n\u001B[32m   1987\u001B[39m                 chunk_value = chunk_value[item]\n\u001B[32m   1989\u001B[39m         \u001B[38;5;66;03m# put data\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1990\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_chunk_setitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk_coords\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_selection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_value\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfields\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfields\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1991\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1992\u001B[39m     lchunk_coords, lchunk_selection, lout_selection = \u001B[38;5;28mzip\u001B[39m(*indexer)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/her2-class/lib/python3.12/site-packages/zarr/core.py:2263\u001B[39m, in \u001B[36mArray._chunk_setitem\u001B[39m\u001B[34m(self, chunk_coords, chunk_selection, value, fields)\u001B[39m\n\u001B[32m   2260\u001B[39m     lock = \u001B[38;5;28mself\u001B[39m._synchronizer[ckey]\n\u001B[32m   2262\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m lock:\n\u001B[32m-> \u001B[39m\u001B[32m2263\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_chunk_setitem_nosync\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk_coords\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_selection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfields\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfields\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/her2-class/lib/python3.12/site-packages/zarr/core.py:2273\u001B[39m, in \u001B[36mArray._chunk_setitem_nosync\u001B[39m\u001B[34m(self, chunk_coords, chunk_selection, value, fields)\u001B[39m\n\u001B[32m   2271\u001B[39m     \u001B[38;5;28mself\u001B[39m._chunk_delitem(ckey)\n\u001B[32m   2272\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2273\u001B[39m     \u001B[38;5;28mself\u001B[39m.chunk_store[ckey] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_encode_chunk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/her2-class/lib/python3.12/site-packages/zarr/core.py:2397\u001B[39m, in \u001B[36mArray._encode_chunk\u001B[39m\u001B[34m(self, chunk)\u001B[39m\n\u001B[32m   2395\u001B[39m \u001B[38;5;66;03m# compress\u001B[39;00m\n\u001B[32m   2396\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compressor:\n\u001B[32m-> \u001B[39m\u001B[32m2397\u001B[39m     cdata = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_compressor\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2398\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2399\u001B[39m     cdata = chunk\n",
      "\u001B[36mFile \u001B[39m\u001B[32mnumcodecs/blosc.pyx:581\u001B[39m, in \u001B[36mnumcodecs.blosc.Blosc.encode\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mnumcodecs/blosc.pyx:307\u001B[39m, in \u001B[36mnumcodecs.blosc.compress\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/her2-class/lib/python3.12/multiprocessing/synchronize.py:97\u001B[39m, in \u001B[36mSemLock.__exit__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m     94\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__enter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m     95\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._semlock.\u001B[34m__enter__\u001B[39m()\n\u001B[32m---> \u001B[39m\u001B[32m97\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args):\n\u001B[32m     98\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._semlock.\u001B[34m__exit__\u001B[39m(*args)\n\u001B[32m    100\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getstate__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Create Train/Val Split Manifest"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "zarr_files = list(Path(ZARR_OUTPUT_DIR).glob(\"*.zarr\"))\n",
    "print(f\"Found {len(zarr_files)} Zarr files\")\n",
    "\n",
    "if len(zarr_files) > 0:\n",
    "    zarr_manifest = []\n",
    "    for zarr_path in tqdm(zarr_files, desc=\"Reading Zarr metadata\"):\n",
    "        meta_path = zarr_path / \"meta.json\"\n",
    "        if meta_path.exists():\n",
    "            with open(meta_path, 'r') as f:\n",
    "                meta = json.load(f)\n",
    "            zarr_manifest.append({\n",
    "                'zarr_path': str(zarr_path),\n",
    "                'slide_id': meta.get('slide_id', zarr_path.stem),\n",
    "                'label': meta.get('label', 0),\n",
    "                'num_patches': meta.get('num_patches', 0)\n",
    "            })\n",
    "    df_zarr = pd.DataFrame(zarr_manifest)\n",
    "    print(f\"\\n✓ Loaded metadata for {len(df_zarr)} Zarr files\")\n",
    "    print(f\"Total patches: {df_zarr['num_patches'].sum():,}\")\n",
    "    if len(df_zarr) > 1 and df_zarr['label'].nunique() > 1:\n",
    "        train_df, val_df = train_test_split(df_zarr, test_size=0.2, stratify=df_zarr['label'], random_state=42)\n",
    "    else:\n",
    "        train_df, val_df = train_test_split(df_zarr, test_size=0.2, random_state=42)\n",
    "    train_manifest_path = f\"{OUTPUTS_ROOT}/zarr_train_manifest.csv\"\n",
    "    val_manifest_path = f\"{OUTPUTS_ROOT}/zarr_val_manifest.csv\"\n",
    "    train_df.to_csv(train_manifest_path, index=False)\n",
    "    val_df.to_csv(val_manifest_path, index=False)\n",
    "    print(f\"\\n✓ Train manifest saved: {train_manifest_path}\")\n",
    "    print(f\"✓ Val manifest saved: {val_manifest_path}\")\n",
    "else:\n",
    "    print(\"⚠️  No Zarr files found! Preprocessing may have failed.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Verify Zarr Loading and Visualization"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "zarr_files = list(Path(ZARR_OUTPUT_DIR).glob(\"*.zarr\"))\n",
    "if len(zarr_files) > 0:\n",
    "    test_zarr_path = zarr_files[0]\n",
    "    print(f\"Testing Zarr: {test_zarr_path.name}\")\n",
    "    z = zarr.open(str(test_zarr_path), mode='r')\n",
    "    print(f\"patches: {z['patches'].shape} {z['patches'].dtype}\")\n",
    "    print(f\"coords: {z['coords'].shape} {z['coords'].dtype}\")\n",
    "    print(f\"labels: {z['labels'].shape} {z['labels'].dtype}\")\n",
    "    meta_path = test_zarr_path / \"meta.json\"\n",
    "    if meta_path.exists():\n",
    "        with open(meta_path, 'r') as f:\n",
    "            meta = json.load(f)\n",
    "        print(f\"Meta: slide={meta['slide_id']}, label={meta['label']}, num_patches={meta['num_patches']}\")\n",
    "    # Show a grid of patches\n",
    "    n = min(12, z['patches'].shape[0])\n",
    "    if n > 0:\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "        axes = axes.flatten()\n",
    "        for i in range(n):\n",
    "            axes[i].imshow(z['patches'][i])\n",
    "            axes[i].set_title(f\"{i}\")\n",
    "            axes[i].axis('off')\n",
    "        for j in range(n, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"⚠️  No Zarr files found for testing\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Summary"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PREPROCESSING PIPELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n1. Reference Stain Normalization\")\n",
    "if Path(REF_STAIN_STATS_PATH).exists():\n",
    "    print(f\"   ✓ Reference stats: {REF_STAIN_STATS_PATH}\")\n",
    "else:\n",
    "    print(f\"   ✗ Reference stats not found\")\n",
    "\n",
    "print(f\"\\n2. Zarr Files Created\")\n",
    "zarr_files = list(Path(ZARR_OUTPUT_DIR).glob(\"*.zarr\"))\n",
    "print(f\"   Total Zarr files: {len(zarr_files)}\")\n",
    "if zarr_files:\n",
    "    total_patches = 0\n",
    "    for zf in zarr_files:\n",
    "        meta_path = zf / \"meta.json\"\n",
    "        if meta_path.exists():\n",
    "            with open(meta_path, 'r') as f:\n",
    "                meta = json.load(f)\n",
    "                total_patches += meta.get('num_patches', 0)\n",
    "    print(f\"   Total patches: {total_patches:,}\")\n",
    "\n",
    "print(f\"\\n3. Train/Val Manifests\")\n",
    "train_manifest = Path(f\"{OUTPUTS_ROOT}/zarr_train_manifest.csv\")\n",
    "val_manifest = Path(f\"{OUTPUTS_ROOT}/zarr_val_manifest.csv\")\n",
    "print(f\"   Train manifest: {'✓' if train_manifest.exists() else '✗'} {train_manifest}\")\n",
    "print(f\"   Val manifest: {'✓' if val_manifest.exists() else '✗'} {val_manifest}\")\n",
    "\n",
    "print(\"\\n4. Next Steps: Use PatchDataset to train ResNet-50 / EfficientNet-B0\")\n",
    "print(\"=\"*60)\n",
    "print(\"✅ Preprocessing pipeline complete!\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
