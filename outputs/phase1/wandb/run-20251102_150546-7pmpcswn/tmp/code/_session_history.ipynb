{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feed0491",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from glob import glob\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Install openpyxl if needed for reading Excel files\n",
    "try:\n",
    "    import openpyxl\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openpyxl\"])\n",
    "    import openpyxl\n",
    "\n",
    "# Import dependency\n",
    "from src.preprocessing.generate_metadata import discover_wsi\n",
    "from src.preprocessing.xml_to_mask import get_mask\n",
    "from src.preprocessing.annotation_utils import resolve_annotation_path\n",
    "from src.preprocessing.extract_patches import process_slide\n",
    "from src.preprocessing.load_wsi import load_wsi\n",
    "from src.train.train_phase1 import train_phase1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4fbcb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_DIR = 'data'\n",
    "SOURCES = [\n",
    "    'Yale_HER2_cohort',\n",
    "    'Yale_trastuzumab_response_cohort',\n",
    "    'TCGA_BRCA_Filtered'\n",
    "]\n",
    "OUTPUT_CSV = 'outputs/index/wsi_index.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18611744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "log_dir = 'outputs/preprocessing/logs'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_path = os.path.join(log_dir, 'preprocessing.log')\n",
    "\n",
    "# Configure logging to file only (no console output in notebook)\n",
    "logger = logging.getLogger('preprocessing')\n",
    "if not logger.handlers:\n",
    "    handler = logging.FileHandler(log_path)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # Prevent propagation to avoid duplicate logs\n",
    "    logger.propagate = False\n",
    "\n",
    "def log(msg):\n",
    "    \"\"\"Log message using Python logging module (proper file handling).\"\"\"\n",
    "    logger.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60755bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patch_validator(min_std: float = 5.0, min_foreground_ratio: float = 0.02, background_value: int = 245):\n",
    "    \"\"\"Return a validator that drops low-contrast or mostly background patches.\"\"\"\n",
    "    stats = {'accepted': 0, 'discarded': 0}\n",
    "    def _validator(patch, meta):\n",
    "        arr = np.asarray(patch)\n",
    "        if arr.ndim == 3:\n",
    "            gray = cv2.cvtColor(arr, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray = arr\n",
    "        if float(gray.std()) < min_std:\n",
    "            stats['discarded'] += 1\n",
    "            return False\n",
    "        foreground_ratio = float(np.mean(gray < background_value))\n",
    "        if foreground_ratio < min_foreground_ratio:\n",
    "            stats['discarded'] += 1\n",
    "            return False\n",
    "        stats['accepted'] += 1\n",
    "        return True\n",
    "    _validator.stats = stats\n",
    "    return _validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9991a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = discover_wsi(\n",
    "    base_dir=BASE_DIR, \n",
    "    sources=SOURCES, \n",
    "    output_path=OUTPUT_CSV\n",
    ")\n",
    "\n",
    "# Load and display the results\n",
    "df = pd.read_csv(csv_path)\n",
    "display(df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3dfdf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tqdm(df.head(200).itertuples(index=False), total=200, desc='Processing slides'):\n",
    "    process_slide(row, base_dir=BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d96acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "log(\"=\" * 80)\n",
    "log(\"Starting train/val CSV generation\")\n",
    "log(\"=\" * 80)\n",
    "\n",
    "# Load TCGA label mapping from Excel file\n",
    "tcga_excel_path = 'data/TCGA_BRCA_Filtered/case&annotation_counts_clean.xlsx'\n",
    "tcga_labels = {}\n",
    "\n",
    "if os.path.exists(tcga_excel_path):\n",
    "    tcga_df = pd.read_excel(tcga_excel_path)\n",
    "    log(f\"Loaded TCGA labels from: {tcga_excel_path}\")\n",
    "    log(f\"TCGA cases in Excel: {len(tcga_df)}\")\n",
    "    print(f\"Loaded TCGA labels from: {tcga_excel_path}\")\n",
    "    print(f\"TCGA cases in Excel: {len(tcga_df)}\")\n",
    "    \n",
    "    # Create mapping from slide ID to label based on Clinical.HER2.status\n",
    "    for idx, row in tqdm(tcga_df.iterrows(), total=len(tcga_df), desc='Processing TCGA labels'):\n",
    "        slide_id = str(row['Slide']).strip()\n",
    "        her2_status = str(row['Clinical.HER2.status']).strip()\n",
    "        \n",
    "        # Map Clinical.HER2.status to label\n",
    "        if her2_status == 'Positive':\n",
    "            label = 1  # Positive\n",
    "        elif her2_status == 'Negative':\n",
    "            label = 0  # Negative\n",
    "        else:\n",
    "            msg = f\"Warning: Unknown HER2 status '{her2_status}' for {slide_id}\"\n",
    "            log(msg)\n",
    "            print(msg)\n",
    "            continue\n",
    "        \n",
    "        tcga_labels[slide_id] = label\n",
    "    \n",
    "    pos_count = sum(1 for v in tcga_labels.values() if v == 1)\n",
    "    neg_count = sum(1 for v in tcga_labels.values() if v == 0)\n",
    "    log(f\"TCGA positive cases: {pos_count}\")\n",
    "    log(f\"TCGA negative cases: {neg_count}\")\n",
    "    print(f\"TCGA positive cases: {pos_count}\")\n",
    "    print(f\"TCGA negative cases: {neg_count}\")\n",
    "else:\n",
    "    msg = f\"Warning: TCGA Excel file not found at {tcga_excel_path}\"\n",
    "    log(msg)\n",
    "    print(msg)\n",
    "\n",
    "# Collect all patches with labels\n",
    "log(\"Collecting patches from all cases...\")\n",
    "patch_data = []\n",
    "patches_dir = 'outputs/patches'\n",
    "\n",
    "# Get all case directories first\n",
    "case_dirs = [d for d in glob(os.path.join(patches_dir, '*')) if os.path.isdir(d)]\n",
    "log(f\"Found {len(case_dirs)} case directories\")\n",
    "\n",
    "skipped_tcga = []\n",
    "skipped_unknown = []\n",
    "\n",
    "for case_dir in tqdm(case_dirs, desc='Collecting patches from cases'):\n",
    "    case_name = os.path.basename(case_dir)\n",
    "    \n",
    "    # Determine label based on case type\n",
    "    if case_name.startswith('TCGA-'):\n",
    "        # TCGA cases: look up in Excel file\n",
    "        # Extract TCGA slide ID (before the UUID)\n",
    "        # Format: TCGA-A1-A0SP-01Z-00-DX1.UUID\n",
    "        tcga_id = case_name.split('.')[0] if '.' in case_name else case_name\n",
    "        \n",
    "        if tcga_id in tcga_labels:\n",
    "            label = tcga_labels[tcga_id]\n",
    "        else:\n",
    "            msg = f\"TCGA case {tcga_id} not found in Excel, skipping\"\n",
    "            skipped_tcga.append(tcga_id)\n",
    "            tqdm.write(f\"Warning: {msg}\")\n",
    "            continue\n",
    "    \n",
    "    elif case_name.startswith('S') or case_name.startswith('O'):\n",
    "        # Yale cases starting with S-* or O-* are positive\n",
    "        # e.g., S16-32975, O09-03495\n",
    "        label = 1  # Positive\n",
    "    \n",
    "    else:\n",
    "        # Other Yale cases: determine from directory name\n",
    "        if 'Her2Pos' in case_name or 'Pos' in case_name:\n",
    "            label = 1  # Positive\n",
    "        elif 'Her2Neg' in case_name or 'Neg' in case_name:\n",
    "            label = 0  # Negative\n",
    "        else:\n",
    "            msg = f\"Cannot determine label for {case_name}, skipping\"\n",
    "            skipped_unknown.append(case_name)\n",
    "            tqdm.write(f\"Warning: {msg}\")\n",
    "            continue\n",
    "    \n",
    "    # Get all PNG files in this directory\n",
    "    patch_files = glob(os.path.join(case_dir, '*.png'))\n",
    "    \n",
    "    for patch_file in patch_files:\n",
    "        patch_data.append({\n",
    "            'path': patch_file,\n",
    "            'label': label,\n",
    "            'case': case_name\n",
    "        })\n",
    "\n",
    "# Log skipped cases\n",
    "if skipped_tcga:\n",
    "    log(f\"Skipped {len(skipped_tcga)} TCGA cases not found in Excel: {', '.join(skipped_tcga[:5])}{'...' if len(skipped_tcga) > 5 else ''}\")\n",
    "if skipped_unknown:\n",
    "    log(f\"Skipped {len(skipped_unknown)} cases with unknown labels: {', '.join(skipped_unknown[:5])}{'...' if len(skipped_unknown) > 5 else ''}\")\n",
    "\n",
    "log(f\"Total patches collected: {len(patch_data)}\")\n",
    "print(f\"\\nTotal patches found: {len(patch_data)}\")\n",
    "\n",
    "# Create DataFrame\n",
    "patches_df = pd.DataFrame(patch_data)\n",
    "\n",
    "# Display label distribution\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(patches_df['label'].value_counts())\n",
    "neg_patches = (patches_df['label']==0).sum()\n",
    "pos_patches = (patches_df['label']==1).sum()\n",
    "print(f\"\\nNegative (0): {neg_patches}\")\n",
    "print(f\"Positive (1): {pos_patches}\")\n",
    "log(f\"Label distribution - Negative: {neg_patches}, Positive: {pos_patches}\")\n",
    "\n",
    "# Display cases per label\n",
    "cases_by_label = patches_df.groupby('label')['case'].nunique()\n",
    "neg_cases = cases_by_label.get(0, 0)\n",
    "pos_cases = cases_by_label.get(1, 0)\n",
    "print(f\"\\nNumber of cases:\")\n",
    "print(f\"Negative cases: {neg_cases}\")\n",
    "print(f\"Positive cases: {pos_cases}\")\n",
    "log(f\"Number of cases - Negative: {neg_cases}, Positive: {pos_cases}\")\n",
    "\n",
    "# Split by case to avoid data leakage (patches from same slide stay together)\n",
    "log(\"Performing train/val split by case...\")\n",
    "cases = patches_df['case'].unique()\n",
    "train_cases, val_cases = train_test_split(\n",
    "    cases, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=[patches_df[patches_df['case']==c]['label'].iloc[0] for c in cases]\n",
    ")\n",
    "\n",
    "train_df = patches_df[patches_df['case'].isin(train_cases)][['path', 'label']]\n",
    "val_df = patches_df[patches_df['case'].isin(val_cases)][['path', 'label']]\n",
    "\n",
    "log(f\"Train patches: {len(train_df)}, Val patches: {len(val_df)}\")\n",
    "log(f\"Train cases: {len(train_cases)}, Val cases: {len(val_cases)}\")\n",
    "print(f\"\\nTrain patches: {len(train_df)}\")\n",
    "print(f\"Val patches: {len(val_df)}\")\n",
    "print(f\"Train cases: {len(train_cases)}\")\n",
    "print(f\"Val cases: {len(val_cases)}\")\n",
    "\n",
    "# Display split balance\n",
    "train_neg = (train_df['label']==0).sum()\n",
    "train_pos = (train_df['label']==1).sum()\n",
    "val_neg = (val_df['label']==0).sum()\n",
    "val_pos = (val_df['label']==1).sum()\n",
    "\n",
    "print(f\"\\nTrain label distribution:\")\n",
    "print(train_df['label'].value_counts())\n",
    "print(f\"\\nVal label distribution:\")\n",
    "print(val_df['label'].value_counts())\n",
    "\n",
    "log(f\"Train split - Negative: {train_neg}, Positive: {train_pos}\")\n",
    "log(f\"Val split - Negative: {val_neg}, Positive: {val_pos}\")\n",
    "\n",
    "# Save CSV files\n",
    "train_csv_path = 'outputs/patches_index_train.csv'\n",
    "val_csv_path = 'outputs/patches_index_val.csv'\n",
    "\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "val_df.to_csv(val_csv_path, index=False)\n",
    "\n",
    "log(f\"Saved train CSV to: {train_csv_path}\")\n",
    "log(f\"Saved val CSV to: {val_csv_path}\")\n",
    "log(\"Train/val CSV generation completed successfully\")\n",
    "log(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nSaved train CSV to: {train_csv_path}\")\n",
    "print(f\"Saved val CSV to: {val_csv_path}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample from train set:\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44010302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "checkpoint_dir = Path('outputs/phase1/models')\n",
    "checkpoint_last = checkpoint_dir / 'checkpoint_last.pth'\n",
    "\n",
    "# Check checkpoint status\n",
    "if checkpoint_last.exists():\n",
    "    import torch\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_last, map_location='cpu')\n",
    "        print(f\"✓ Checkpoint found: {checkpoint_last}\")\n",
    "        print(f\"  - Epoch: {checkpoint['epoch']}\")\n",
    "        print(f\"  - Best score ({checkpoint.get('best_metrics', {}).get('score_key', 'auc')}): {checkpoint['best_score']:.4f}\")\n",
    "        print(f\"  - Training will resume from epoch {checkpoint['epoch'] + 1}\")\n",
    "        \n",
    "        # Find all periodic checkpoints\n",
    "        periodic_checkpoints = sorted(checkpoint_dir.glob('checkpoint_epoch_*.pth'))\n",
    "        if periodic_checkpoints:\n",
    "            print(f\"\\n  Periodic checkpoints ({len(periodic_checkpoints)}):\")\n",
    "            for cp in periodic_checkpoints[-3:]:  # Show last 3\n",
    "                print(f\"    - {cp.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Checkpoint exists but cannot be loaded: {e}\")\n",
    "        print(f\"  Consider deleting corrupted checkpoint: rm {checkpoint_last}\")\n",
    "else:\n",
    "    print(f\"✗ No checkpoint found at {checkpoint_last}\")\n",
    "    print(f\"  Training will start from scratch\")\n",
    "\n",
    "# Helper function to clear checkpoints (uncomment to use)\n",
    "def clear_checkpoints():\n",
    "    \"\"\"Remove all checkpoint files to start fresh\"\"\"\n",
    "    checkpoint_files = list(checkpoint_dir.glob('checkpoint_*.pth'))\n",
    "    for cp in checkpoint_files:\n",
    "        cp.unlink()\n",
    "        print(f\"Deleted: {cp.name}\")\n",
    "    if checkpoint_files:\n",
    "        print(f\"\\nCleared {len(checkpoint_files)} checkpoint(s)\")\n",
    "    else:\n",
    "        print(\"No checkpoints to clear\")\n",
    "\n",
    "# Uncomment the line below to clear all checkpoints and start fresh\n",
    "# clear_checkpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b0139dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>outputs/phase1/wandb/run-20251102_150546-7pmpcswn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/thanakornbua/her2-classification/runs/7pmpcswn' target=\"_blank\">phase1_resnet50_bs8_size512_amp</a></strong> to <a href='http://localhost:8080/thanakornbua/her2-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/thanakornbua/her2-classification' target=\"_blank\">http://localhost:8080/thanakornbua/her2-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/thanakornbua/her2-classification/runs/7pmpcswn' target=\"_blank\">http://localhost:8080/thanakornbua/her2-classification/runs/7pmpcswn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set CUDA memory optimization environment variable\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Clear any existing CUDA cache\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Check if wandb is installed\n",
    "try:\n",
    "    import wandb\n",
    "    print(f\"Weights & Biases version: {wandb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Warning: wandb not installed. Install with: pip install wandb\")\n",
    "    print(\"Will continue with TensorBoard logging only\")\n",
    "\n",
    "# CSV Detail: path (image path), label (0=negative, 1=positive)\n",
    "# Memory optimizations for 7.6GB GPU:\n",
    "# - Input size: 512x512 (high resolution for better feature extraction)\n",
    "# - Reduced batch_size: 32 -> 8 (4x less memory)\n",
    "# - Gradient accumulation: 4 steps (simulates batch_size=32)\n",
    "# - Mixed precision (FP16): ~50% memory reduction\n",
    "# - Aggressive cache clearing: Every 10 batches\n",
    "# Note: 512x512 uses more memory than 224x224. Monitor GPU usage. Reduce to 224 if OOM occurs.\n",
    "\n",
    "CFG = {\n",
    "    'train_csv': 'outputs/patches_index_train.csv',\n",
    "    'val_csv': 'outputs/patches_index_val.csv',\n",
    "    'output_dir': 'outputs/phase1',\n",
    "    'pretrained': True,\n",
    "    'input_size': 512,  # Full resolution patches\n",
    "    'batch_size': 8,    # Reduced from 32 to 8\n",
    "    'accumulation_steps': 4,  # Gradient accumulation to simulate batch_size=32\n",
    "    'use_amp': True,    # Mixed precision training (FP16) for memory efficiency\n",
    "    'num_workers': 2,   # Reduced to limit CPU memory\n",
    "    'epochs': 10,\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'label_col': 'label',\n",
    "    'path_col': 'path',\n",
    "    'save_best_by': 'auc',\n",
    "    'seed': 42,\n",
    "    # Experiment tracking\n",
    "    'use_wandb': True,  # Enable Weights & Biases logging\n",
    "    'wandb_project': 'her2-classification',\n",
    "    'wandb_name': f'phase1_resnet50_bs{8}_size{512}_amp',\n",
    "    # Checkpoint resumption\n",
    "    'resume': True,  # Automatically resume from last checkpoint if available\n",
    "}\n",
    "\n",
    "log(\"=\" * 80)\n",
    "log(\"Starting Phase 1 training (ResNet-50)\")\n",
    "log(f\"Configuration: {CFG}\")\n",
    "log(f\"Memory optimizations enabled:\")\n",
    "log(f\"  - Input size: {CFG['input_size']}x{CFG['input_size']}\")\n",
    "log(f\"  - Batch size: {CFG['batch_size']} with {CFG.get('accumulation_steps', 1)} accumulation steps\")\n",
    "log(f\"  - Effective batch size: {CFG['batch_size'] * CFG.get('accumulation_steps', 1)}\")\n",
    "log(f\"  - Mixed precision (FP16): {CFG.get('use_amp', False)}\")\n",
    "log(f\"Experiment tracking:\")\n",
    "log(f\"  - TensorBoard: Enabled\")\n",
    "log(f\"  - Weights & Biases: {CFG.get('use_wandb', False)}\")\n",
    "log(f\"Checkpoint resumption: {CFG.get('resume', False)}\")\n",
    "log(\"=\" * 80)\n",
    "\n",
    "results = train_phase1(CFG)\n",
    "\n",
    "log(f\"Training completed - Best model: {results['best_model_path']}\")\n",
    "log(f\"Best metrics: {results['best_metrics']}\")\n",
    "log(f\"TensorBoard logs: {results['tb_dir']}\")\n",
    "if results.get('wandb_url'):\n",
    "    log(f\"Weights & Biases URL: {results['wandb_url']}\")\n",
    "log(\"=\" * 80)\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('TRAINING COMPLETED')\n",
    "print('='*80)\n",
    "print(f\"Best model: {results['best_model_path']}\")\n",
    "print(f\"Logs dir: {results['logs_dir']}\")\n",
    "print(f\"TensorBoard dir: {results['tb_dir']}\")\n",
    "print('\\nTo view TensorBoard:')\n",
    "print(f\"  tensorboard --logdir={results['tb_dir']}\")\n",
    "if results.get('wandb_url'):\n",
    "    print('\\nWeights & Biases Dashboard:')\n",
    "    print(f\"  {results['wandb_url']}\")\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa679d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA memory optimization environment variable\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Clear any existing CUDA cache\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "# Check if wandb is installed\n",
    "try:\n",
    "    import wandb\n",
    "    print(f\"Weights & Biases version: {wandb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Warning: wandb not installed. Install with: pip install wandb\")\n",
    "    print(\"Will continue with TensorBoard logging only\")\n",
    "\n",
    "# CSV Detail: path (image path), label (0=negative, 1=positive)\n",
    "# Memory optimizations for 7.6GB GPU:\n",
    "# - Input size: 512x512 (high resolution for better feature extraction)\n",
    "# - Reduced batch_size: 32 -> 8 (4x less memory)\n",
    "# - Gradient accumulation: 4 steps (simulates batch_size=32)\n",
    "# - Mixed precision (FP16): ~50% memory reduction\n",
    "# - Aggressive cache clearing: Every 10 batches\n",
    "# Note: 512x512 uses more memory than 224x224. Monitor GPU usage. Reduce to 224 if OOM occurs.\n",
    "\n",
    "CFG = {\n",
    "    'train_csv': 'outputs/patches_index_train.csv',\n",
    "    'val_csv': 'outputs/patches_index_val.csv',\n",
    "    'output_dir': 'outputs/phase1',\n",
    "    'pretrained': True,\n",
    "    'input_size': 512,  # Full resolution patches\n",
    "    'batch_size': 8,    # Reduced from 32 to 8\n",
    "    'accumulation_steps': 4,  # Gradient accumulation to simulate batch_size=32\n",
    "    'use_amp': True,    # Mixed precision training (FP16) for memory efficiency\n",
    "    'num_workers': 2,   # Reduced to limit CPU memory\n",
    "    'epochs': 10,\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'label_col': 'label',\n",
    "    'path_col': 'path',\n",
    "    'save_best_by': 'auc',\n",
    "    'seed': 42,\n",
    "    # Experiment tracking\n",
    "    'use_wandb': True,  # Enable Weights & Biases logging\n",
    "    'wandb_project': 'her2-classification',\n",
    "    'wandb_name': f'phase1_resnet50_bs{8}_size{512}_amp',\n",
    "    # Checkpoint resumption\n",
    "    'resume': True,  # Automatically resume from last checkpoint if available\n",
    "}\n",
    "\n",
    "log(\"=\" * 80)\n",
    "log(\"Starting Phase 1 training (ResNet-50)\")\n",
    "log(f\"Configuration: {CFG}\")\n",
    "log(f\"Memory optimizations enabled:\")\n",
    "log(f\"  - Input size: {CFG['input_size']}x{CFG['input_size']}\")\n",
    "log(f\"  - Batch size: {CFG['batch_size']} with {CFG.get('accumulation_steps', 1)} accumulation steps\")\n",
    "log(f\"  - Effective batch size: {CFG['batch_size'] * CFG.get('accumulation_steps', 1)}\")\n",
    "log(f\"  - Mixed precision (FP16): {CFG.get('use_amp', False)}\")\n",
    "log(f\"Experiment tracking:\")\n",
    "log(f\"  - TensorBoard: Enabled\")\n",
    "log(f\"  - Weights & Biases: {CFG.get('use_wandb', False)}\")\n",
    "log(f\"Checkpoint resumption: {CFG.get('resume', False)}\")\n",
    "log(\"=\" * 80)\n",
    "\n",
    "results = train_phase1(CFG)\n",
    "\n",
    "log(f\"Training completed - Best model: {results['best_model_path']}\")\n",
    "log(f\"Best metrics: {results['best_metrics']}\")\n",
    "log(f\"TensorBoard logs: {results['tb_dir']}\")\n",
    "if results.get('wandb_url'):\n",
    "    log(f\"Weights & Biases URL: {results['wandb_url']}\")\n",
    "log(\"=\" * 80)\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('TRAINING COMPLETED')\n",
    "print('='*80)\n",
    "print(f\"Best model: {results['best_model_path']}\")\n",
    "print(f\"Logs dir: {results['logs_dir']}\")\n",
    "print(f\"TensorBoard dir: {results['tb_dir']}\")\n",
    "print('\\nTo view TensorBoard:')\n",
    "print(f\"  tensorboard --logdir={results['tb_dir']}\")\n",
    "if results.get('wandb_url'):\n",
    "    print('\\nWeights & Biases Dashboard:')\n",
    "    print(f\"  {results['wandb_url']}\")\n",
    "print('='*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
